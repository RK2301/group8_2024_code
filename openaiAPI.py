from openai import OpenAI
import os
import json
import pandas as pd
import numpy as np

secrectKey = 'api_key_key_should_be_here'
client = OpenAI(
    api_key = secrectKey,
)

def map_back_keywords(model_answer, keyword_mappings):
    """
    Maps the keywords in the model's answer back to the original library keywords using the provided mappings.

    Parameters:
    - model_answer (str): The code generated by the model using the private library.
    - keyword_mappings (dict): A dictionary with the private keywords as keys and the original keywords as values.

    Returns:
    - str: The model's answer with keywords mapped back to the original library keywords.
    """
    # Reverse the keyword_mappings dictionary to map back to the original keywords
    reverse_mapping = {v: k for k, v in keyword_mappings.items()}

    # Sort the reverse mapping by key length in descending order to avoid partial replacements
    sorted_mapping = sorted(reverse_mapping.items(), key=lambda x: len(x[0]), reverse=True)

    # Replace keywords in the model's answer
    for private_keyword, original_keyword in sorted_mapping:
        model_answer = model_answer.replace(private_keyword, original_keyword)
    
    return model_answer

#get list of mapping from pandas to monkey
with open(r'apiDocumentation\pandas\keywords_mapping.json', 'r') as file:
    mapping_pd_to_mk = json.load(file)


# Load API Documentation from JSON file, monkey documentation
with open(r'apiDocumentation\pandas\pandas_private.json', 'r') as monkey_doc:
    monkey_doc = json.load(monkey_doc)


# Load Benchmarks from JSON file
with open(r'benchmarks\monkey_bm.json', 'r') as monkey_bm:
    monkey_benchmarks = json.load(monkey_bm)

# Extract the prompts from the benchmarks
prompts = [bm["prompt"] for bm in monkey_benchmarks["benchmarks"]]

full_prompt_monkey = {
    'api_documentation': monkey_doc,
    'benchmarks': prompts
}

full_prompt_monkey = json.dumps(full_prompt_monkey, indent=4)

##################################################Files For beatnum###############################
#get list of mapping from numpy to beatnum
with open(r'apiDocumentation\numpy\keywords_mapping.json', 'r') as beatnum_mapping:
    mapping_np_to_bn = json.load(beatnum_mapping)


# Load API Documentation from JSON file, beatnum documentation
with open(r'apiDocumentation\numpy\numpy_private.json', 'r') as beatnumDoc:
    beatnum_doc = json.load(beatnumDoc)


# Load Benchmarks from JSON file
with open(r'benchmarks\beatnum_bm.json', 'r') as beatnumBM:
    beatnum_bm = json.load(beatnumBM)

# Extract the prompts from the benchmarks
beatnum_prompts = [bm["prompt"] for bm in beatnum_bm["benchmarks"]]

#the prompt that will sent to openai api
full_prompt_beatnum = {
    'api_documentation': beatnum_doc,
    'benchmarks': beatnum_prompts
}

# Convert to JSON string format for sending
full_prompt_beatnum = json.dumps(full_prompt_beatnum, indent=4)

#test the becnhmarks on gpt-4mini, after recieve the response test the correctenes of each answer for becnhmark
def test_gpt_4mini(full_prompt, map_list, benchmarks):
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You will recieve api_documentation for private library, and benchmarks as JSON format. the benchmarks will consist of array of prompts, each prompt contain function signature (name and parameters) and must complete the body of the function based on the description in the prompt. the goal to test your performance to complete the function body using private library (based on the documentation you will get). your response should be in JSON format, contain array each index has the completion for the function body with it's signature only (no explation or instructions). here an example for you response: {'answers': ['def add_zeros_to_string(kf, col_name):\n    kf[col_name] = kf[col_name].employ(lambda x: '{0:0>15}'.format(x))\n    return kf']}, if you recieved one benchmark. Note: please response only with json format, so i can parse it easily in my code, like don't add ```json ...."
            },
            {
                "role": "user", 
                "content": full_prompt
            }
        ]
    )

    # Extracting the response
    response_text = completion.choices[0].message.content
    #print(response_text)

    json_res = json.loads(response_text)
    answers = json_res['answers']

    count_corr = 0
    #for each response from the openai api
    #we will extract the function and map it back to it's original library (since we didn't create private version)
    #extract the proper test for the becnhmark
    #check the correctnes of the generated function
    for idx, func in enumerate(answers):

        ans_func = map_back_keywords(func, map_list)
        # Extract the corresponding test from benchmarks
        test_code = benchmarks['benchmarks'][idx]['test']

        try:
            exec(ans_func)
            exec(test_code)

            #extract function name
            function_name = func.split(' ')[1].split('(')[0]
            re = eval(f"check({function_name})")
            if re:
                count_corr=count_corr + 1
                
        except Exception as e:
            print(f"An error occurred: {e}")

    print('Number of correct answers is: ' + str(count_corr))

test_gpt_4mini(full_prompt_monkey, mapping_pd_to_mk, monkey_benchmarks)
test_gpt_4mini(full_prompt_beatnum, mapping_np_to_bn, beatnum_bm)
